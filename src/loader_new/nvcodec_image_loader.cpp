/* SPDX-FileCopyrightText: 2025 LichtFeld Studio Authors
 *
 * SPDX-License-Identifier: GPL-3.0-or-later */

#include "loader_new/nvcodec_image_loader.hpp"
#include "core_new/logger.hpp"
#include "core_new/tensor.hpp"

#include <nvimgcodec.h>
#include <cuda_runtime.h>
#include <algorithm>
#include <condition_variable>
#include <fstream>
#include <stdexcept>

namespace lfs::loader {

    // Internal implementation (PIMPL pattern to hide nvImageCodec types)
    struct NvCodecImageLoader::Impl {
        nvimgcodecInstance_t instance = nullptr;
        std::vector<nvimgcodecDecoder_t> decoder_pool;  // Pool of decoders (one per thread)
        std::vector<bool> decoder_available;             // Track which decoders are available
        std::mutex pool_mutex;                           // Protect pool access
        std::condition_variable pool_cv;                 // Wait for available decoder
        int device_id = 0;
        bool fallback_enabled = true;

        // Acquire a decoder from the pool (blocks until one is available)
        size_t acquire_decoder() {
            std::unique_lock<std::mutex> lock(pool_mutex);
            pool_cv.wait(lock, [this] {
                return std::find(decoder_available.begin(), decoder_available.end(), true) != decoder_available.end();
            });

            // Find first available decoder
            for (size_t i = 0; i < decoder_available.size(); ++i) {
                if (decoder_available[i]) {
                    decoder_available[i] = false;
                    return i;
                }
            }
            return 0; // Should never reach here
        }

        // Release a decoder back to the pool
        void release_decoder(size_t idx) {
            {
                std::lock_guard<std::mutex> lock(pool_mutex);
                decoder_available[idx] = true;
            }
            pool_cv.notify_one();
        }

        ~Impl() {
            for (auto decoder : decoder_pool) {
                if (decoder) {
                    nvimgcodecDecoderDestroy(decoder);
                }
            }
            if (instance) {
                nvimgcodecInstanceDestroy(instance);
            }
        }
    };

    NvCodecImageLoader::NvCodecImageLoader(const Options& options)
        : impl_(std::make_unique<Impl>()) {

        impl_->device_id = options.device_id;
        impl_->fallback_enabled = options.enable_fallback;

        // Create nvImageCodec instance
        nvimgcodecInstanceCreateInfo_t create_info{NVIMGCODEC_STRUCTURE_TYPE_INSTANCE_CREATE_INFO, sizeof(nvimgcodecInstanceCreateInfo_t), nullptr};
        create_info.load_builtin_modules = 1;
        create_info.load_extension_modules = 1;
        // Colon-separated paths to extension modules (nvjpeg for hardware JPEG decode, libjpeg_turbo for CPU fallback)
        create_info.extension_modules_path =
            "build/external/nvImageCodec/extensions/nvjpeg:"
            "build/external/nvImageCodec/extensions/libjpeg_turbo:"
            "build/external/nvImageCodec/extensions/nvjpeg2k:"
            "build/external/nvImageCodec/extensions/nvbmp";
        create_info.create_debug_messenger = 0;  // Disable debug output for now

        auto status = nvimgcodecInstanceCreate(&impl_->instance, &create_info);
        if (status != NVIMGCODEC_STATUS_SUCCESS) {
            throw std::runtime_error("Failed to create nvImageCodec instance");
        }

        // Create pool of decoders (one per worker thread)
        // Use pool_size based on expected worker threads (default: 16)
        size_t pool_size = options.decoder_pool_size > 0 ? options.decoder_pool_size : 16;
        impl_->decoder_pool.resize(pool_size);
        impl_->decoder_available.resize(pool_size, true);

        // Use GPU backend (nvjpeg) for performance
        // Note: nvjpeg has small memory leaks in CUDA 12.8, but they're tiny per-image
        nvimgcodecExecutionParams_t exec_params{NVIMGCODEC_STRUCTURE_TYPE_EXECUTION_PARAMS, sizeof(nvimgcodecExecutionParams_t), nullptr};
        exec_params.device_id = options.device_id;
        exec_params.max_num_cpu_threads = options.max_num_cpu_threads;
        exec_params.num_backends = 0;  // Allow all backends (GPU preferred)
        exec_params.backends = nullptr;

        for (size_t i = 0; i < pool_size; ++i) {
            status = nvimgcodecDecoderCreate(impl_->instance, &impl_->decoder_pool[i], &exec_params, nullptr);
            if (status != NVIMGCODEC_STATUS_SUCCESS) {
                throw std::runtime_error("Failed to create nvImageCodec decoder " + std::to_string(i));
            }
        }

        LOG_INFO("NvCodecImageLoader initialized with CPU-only decoding (avoiding nvjpeg leak), {} decoders", pool_size);
    }

    NvCodecImageLoader::~NvCodecImageLoader() = default;

    bool NvCodecImageLoader::is_available() {
        // Check if CUDA is available
        int device_count = 0;
        cudaError_t err = cudaGetDeviceCount(&device_count);
        if (err != cudaSuccess || device_count == 0) {
            return false;
        }

        // Try to create a minimal instance
        nvimgcodecInstance_t test_instance = nullptr;
        nvimgcodecInstanceCreateInfo_t create_info{NVIMGCODEC_STRUCTURE_TYPE_INSTANCE_CREATE_INFO, sizeof(nvimgcodecInstanceCreateInfo_t), nullptr};
        create_info.load_builtin_modules = 1;
        create_info.load_extension_modules = 0;

        auto status = nvimgcodecInstanceCreate(&test_instance, &create_info);
        if (status == NVIMGCODEC_STATUS_SUCCESS && test_instance) {
            nvimgcodecInstanceDestroy(test_instance);
            return true;
        }
        return false;
    }

    std::vector<uint8_t> NvCodecImageLoader::read_file(const std::filesystem::path& path) {
        std::ifstream file(path, std::ios::binary | std::ios::ate);
        if (!file) {
            throw std::runtime_error("Failed to open file: " + path.string());
        }

        std::streamsize size = file.tellg();
        file.seekg(0, std::ios::beg);

        std::vector<uint8_t> buffer(size);
        if (!file.read(reinterpret_cast<char*>(buffer.data()), size)) {
            throw std::runtime_error("Failed to read file: " + path.string());
        }

        return buffer;
    }

    lfs::core::Tensor NvCodecImageLoader::load_image_gpu(
        const std::filesystem::path& path,
        int resize_factor,
        int max_width) {

        LOG_DEBUG("NvCodecImageLoader: Loading {}", path.string());

        // Check file extension - nvImageCodec has PNG parser but no PNG decoder
        // Only JPEG, JPEG2000, BMP, TIFF, PNM have decoders
        std::string ext = path.extension().string();
        std::transform(ext.begin(), ext.end(), ext.begin(), ::tolower);

        if (ext == ".png" || ext == ".webp") {
            throw std::runtime_error("PNG/WebP not supported - no decoder available, use CPU fallback");
        }

        // Read file into memory and decode
        auto file_data = read_file(path);
        return load_image_from_memory_gpu(file_data, resize_factor, max_width);
    }

    lfs::core::Tensor NvCodecImageLoader::load_image_from_memory_gpu(
        const std::vector<uint8_t>& jpeg_data,
        int resize_factor,
        int max_width) {

        // Acquire a decoder from the pool (blocks if all busy)
        size_t decoder_idx = impl_->acquire_decoder();
        nvimgcodecDecoder_t decoder = impl_->decoder_pool[decoder_idx];

        // RAII guard to ensure decoder is released even on exception
        struct DecoderGuard {
            NvCodecImageLoader::Impl* impl;
            size_t idx;
            ~DecoderGuard() { impl->release_decoder(idx); }
        } guard{impl_.get(), decoder_idx};

        // Create code stream from memory
        nvimgcodecCodeStream_t code_stream;
        auto status = nvimgcodecCodeStreamCreateFromHostMem(
            impl_->instance, &code_stream, jpeg_data.data(), jpeg_data.size());
        if (status != NVIMGCODEC_STATUS_SUCCESS) {
            throw std::runtime_error("Failed to create code stream from memory");
        }

        // Get image info (dimensions, format)
        nvimgcodecImageInfo_t image_info{NVIMGCODEC_STRUCTURE_TYPE_IMAGE_INFO, sizeof(nvimgcodecImageInfo_t), nullptr};
        status = nvimgcodecCodeStreamGetImageInfo(code_stream, &image_info);
        if (status != NVIMGCODEC_STATUS_SUCCESS) {
            nvimgcodecCodeStreamDestroy(code_stream);
            throw std::runtime_error("Failed to get image info from memory");
        }

        int width = image_info.plane_info[0].width;
        int height = image_info.plane_info[0].height;
        int channels = image_info.num_planes; // RGB = 3 planes

        LOG_DEBUG("Image info: {}x{}, {} channels", width, height, channels);

        // Use tensor library to allocate GPU buffer - benefits from memory pool!
        // Allocate as uint8 tensor [3, H, W] - will be reused by pool
        using namespace lfs::core;
        auto uint8_tensor = Tensor::empty(
            TensorShape({3, static_cast<size_t>(height), static_cast<size_t>(width)}),
            Device::CUDA,
            DataType::UInt8);

        void* gpu_uint8_buffer = uint8_tensor.raw_ptr();
        size_t decoded_size = width * height * 3;

        // Create nvImageCodec image descriptor for GPU buffer
        nvimgcodecImage_t nv_image;
        nvimgcodecImageInfo_t output_info = image_info;
        output_info.sample_format = NVIMGCODEC_SAMPLEFORMAT_P_RGB; // Planar RGB (RRR...GGG...BBB...)
        output_info.color_spec = NVIMGCODEC_COLORSPEC_SRGB;
        output_info.chroma_subsampling = NVIMGCODEC_SAMPLING_444;

        // Configure for planar uint8 RGB - 3 separate planes
        output_info.num_planes = 3;
        size_t plane_size = width * height;

        // R plane
        output_info.plane_info[0].height = height;
        output_info.plane_info[0].width = width;
        output_info.plane_info[0].row_stride = width;
        output_info.plane_info[0].num_channels = 1;
        output_info.plane_info[0].sample_type = NVIMGCODEC_SAMPLE_DATA_TYPE_UINT8;

        // G plane
        output_info.plane_info[1].height = height;
        output_info.plane_info[1].width = width;
        output_info.plane_info[1].row_stride = width;
        output_info.plane_info[1].num_channels = 1;
        output_info.plane_info[1].sample_type = NVIMGCODEC_SAMPLE_DATA_TYPE_UINT8;

        // B plane
        output_info.plane_info[2].height = height;
        output_info.plane_info[2].width = width;
        output_info.plane_info[2].row_stride = width;
        output_info.plane_info[2].num_channels = 1;
        output_info.plane_info[2].sample_type = NVIMGCODEC_SAMPLE_DATA_TYPE_UINT8;

        output_info.buffer_kind = NVIMGCODEC_IMAGE_BUFFER_KIND_STRIDED_DEVICE;
        output_info.buffer = gpu_uint8_buffer;
        output_info.buffer_size = decoded_size;

        status = nvimgcodecImageCreate(impl_->instance, &nv_image, &output_info);
        if (status != NVIMGCODEC_STATUS_SUCCESS) {
            // Note: gpu_uint8_buffer is managed by uint8_tensor, don't manually free it
            nvimgcodecCodeStreamDestroy(code_stream);
            throw std::runtime_error("Failed to create image descriptor");
        }

        // Decode using the acquired decoder
        nvimgcodecDecodeParams_t decode_params{NVIMGCODEC_STRUCTURE_TYPE_DECODE_PARAMS, sizeof(nvimgcodecDecodeParams_t), nullptr};
        decode_params.apply_exif_orientation = 1;

        nvimgcodecFuture_t decode_future;
        status = nvimgcodecDecoderDecode(
            decoder,  // Use decoder from pool
            &code_stream,
            &nv_image,
            1, // batch_size
            &decode_params,
            &decode_future);

        if (status != NVIMGCODEC_STATUS_SUCCESS) {
            // Note: gpu_uint8_buffer is managed by uint8_tensor, don't manually free it
            nvimgcodecImageDestroy(nv_image);
            nvimgcodecCodeStreamDestroy(code_stream);
            throw std::runtime_error("Failed to decode image from memory");
        }

        // Wait for decode to complete (this waits for nvImageCodec's internal operations)
        status = nvimgcodecFutureWaitForAll(decode_future);
        if (status != NVIMGCODEC_STATUS_SUCCESS) {
            nvimgcodecFutureDestroy(decode_future);
            nvimgcodecImageDestroy(nv_image);
            nvimgcodecCodeStreamDestroy(code_stream);
            throw std::runtime_error("Failed to wait for decode completion");
        }

        // Also synchronize CUDA stream to ensure GPU operations are done
        cudaDeviceSynchronize();

        // Get processing status (for single image decode)
        nvimgcodecProcessingStatus_t decode_status;
        size_t status_size = 1;  // We're decoding 1 image
        nvimgcodecFutureGetProcessingStatus(decode_future, &decode_status, &status_size);

        bool decode_success = (decode_status == NVIMGCODEC_PROCESSING_STATUS_SUCCESS);

        // Cleanup nvImageCodec resources (must be done after waiting)
        nvimgcodecFutureDestroy(decode_future);
        nvimgcodecImageDestroy(nv_image);
        nvimgcodecCodeStreamDestroy(code_stream);

        if (!decode_success) {
            // uint8_tensor will be automatically freed when it goes out of scope
            const char* status_str = decode_status == NVIMGCODEC_PROCESSING_STATUS_FAIL ? "FAIL" :
                                     decode_status == NVIMGCODEC_PROCESSING_STATUS_IMAGE_CORRUPTED ? "IMAGE_CORRUPTED" :
                                     decode_status == NVIMGCODEC_PROCESSING_STATUS_CODEC_UNSUPPORTED ? "CODEC_UNSUPPORTED" :
                                     decode_status == NVIMGCODEC_PROCESSING_STATUS_BACKEND_UNSUPPORTED ? "BACKEND_UNSUPPORTED" :
                                     decode_status == NVIMGCODEC_PROCESSING_STATUS_CODESTREAM_UNSUPPORTED ? "CODESTREAM_UNSUPPORTED" : "UNKNOWN";
            throw std::runtime_error(std::string("Decode failed: ") + status_str);
        }

        // nvImageCodec decoded to uint8 planar RGB (RRR...GGG...BBB...)
        // Convert uint8 tensor → float32 and normalize to [0, 1]
        // uint8_tensor is automatically freed and returned to pool after conversion
        auto output_tensor = uint8_tensor.to(DataType::Float32) / 255.0f;

        LOG_DEBUG("Successfully decoded image to GPU: {}x{}x3 uint8→float32 planar", width, height);

        // Apply resize if needed and return GPU tensor directly
        return resize_if_needed(std::move(output_tensor), width, height, resize_factor, max_width);
    }

    lfs::core::Tensor NvCodecImageLoader::resize_if_needed(
        lfs::core::Tensor input,
        int width,
        int height,
        int resize_factor,
        int max_width) {

        // Calculate target dimensions
        int target_width = width;
        int target_height = height;

        if (resize_factor > 1) {
            target_width /= resize_factor;
            target_height /= resize_factor;
        }

        if (max_width > 0 && (width > max_width || height > max_width)) {
            float scale;
            if (width > height) {
                scale = static_cast<float>(max_width) / width;
            } else {
                scale = static_cast<float>(max_width) / height;
            }
            target_width = static_cast<int>(width * scale);
            target_height = static_cast<int>(height * scale);
        }

        // If no resize needed, return as-is
        if (target_width == width && target_height == height) {
            return input;
        }

        LOG_DEBUG("Resizing from {}x{} to {}x{}", width, height, target_width, target_height);

        // TODO: Implement GPU-based bilinear resize using CUDA or NPP
        // For now, just return the input (resize will be handled by fallback)
        LOG_WARN("GPU resize not yet implemented, returning original size");
        return input;
    }

    std::vector<lfs::core::Tensor> NvCodecImageLoader::load_images_batch_gpu(
        const std::vector<std::filesystem::path>& paths,
        int resize_factor,
        int max_width) {

        // Phase 2: TODO - implement batch decoding
        LOG_WARN("Batch loading not yet implemented, falling back to sequential");

        std::vector<lfs::core::Tensor> results;
        results.reserve(paths.size());

        for (const auto& path : paths) {
            results.push_back(load_image_gpu(path, resize_factor, max_width));
        }

        return results;
    }

} // namespace lfs::loader
